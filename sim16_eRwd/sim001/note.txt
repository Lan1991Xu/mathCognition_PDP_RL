Hi, Professor McClelland, 

Now the model is able to learn without intermediate reward. In particular, the model only gets bigPositive reward at the moment when all items were touched, and it gets small negative (-.05) reward otherwise. Here's the learning curve. I trained it for 20,000 epoch. 


Here's the final performance of the model, it is worse than the previous version of model, but it is pretty good. 


I also attached the weights plot, which looks pretty similar to the previous model. 

So I guess this implementation is working. Now, the model is adjusting weights by discounted future reward = currentReward + expectedReward * discountFactor. And expected reward = max(weights*visualInput)

Sincerely,
Q